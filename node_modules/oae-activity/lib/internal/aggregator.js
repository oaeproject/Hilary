/*!
 * Copyright 2012 Sakai Foundation (SF) Licensed under the
 * Educational Community License, Version 2.0 (the "License"); you may
 * not use this file except in compliance with the License. You may
 * obtain a copy of the License at
 *
 *     http://www.osedu.org/licenses/ECL-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an "AS IS"
 * BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
 * or implied. See the License for the specific language governing
 * permissions and limitations under the License.
 */

var _ = require('underscore');
var clone = require('clone');
var util = require('util');

var log = require('oae-logger').logger('oae-activity-aggregator');
var OAE = require('oae-util/lib/oae');
var Telemetry = require('oae-telemetry').telemetry('activity');

var Activity = require('oae-activity/lib/model').Activity;
var ActivityCluster = require('./cluster');
var ActivityConstants = require('oae-activity/lib/constants').ActivityConstants;
var ActivityDAO = require('./dao');
var ActivityEmitter = require('./emitter');
var ActivityEntity = require('oae-activity/lib/model').ActivityEntity;
var ActivityRegistry = require('./registry');
var ActivitySystemConfig = require('./config');

// Used in an aggregate key to denote that there was no entity provided for an activity. This differs from an empty string in that
// an empty string is used when the aggregate does not pivot on that entity.
var ENTITY_KEY_EMPTY = '__null__';

var currentConcurrentCollectionCount = 0;

var shuttingDown = false;

/*!
 * When the system starts shutting down, we want to abort any collections that are happening. As collections will
 * continue until the bucket is empty, we want to ensure it completes within the shutdown grace-time. The only way
 * we can ensure that is to force it to stop after the current batch.
 */
OAE.registerPreShutdownHandler('oae-activity-aggregator', null, function(callback) {
    log().info('Enabling shutdown status to abort any current activity collections as soon as possible.');
    shuttingDown = true;
    return callback();
});

/**
 * Perform a full collection of all activity buckets. If any bucket is already locked by another process, it will be skipped. When
 * this process completes and the callback is invoked, it will guarantee that:
 *
 * a) This process was not allowed to start another collection cycle, as there were too many occuring; or
 * b) for every bucket that wasn't locked, it was collected until it was empty.
 *
 * This is most useful for unit tests to ensure that all activities up until a point in time have been aggregated and delivered.
 *
 * @param  {Function}   [callback]      Invoked when collection is complete.
 * @param  {Object}     [callback.err]  An error that occurred, if any
 */
var collectAllBuckets = module.exports.collectAllBuckets = function(callback) {
    callback = callback || function(err) {
        if (err) {
            log().error({'err': err}, 'Error collecting aggregates.');
        }
    };

    // Ensure we don't surpass the configured number of concurrent collections
    if (currentConcurrentCollectionCount >= ActivitySystemConfig.getConfig().maxConcurrentCollections) {
        log().trace('Aborting collection due to max concurrent collections count reached.');
        return callback();
    }
    currentConcurrentCollectionCount++;

    // Fill all the possible bucket numbers to collect
    var bucketNumbers = [];
    for (var i = 0; i < ActivitySystemConfig.getConfig().numberOfProcessingBuckets; i++) {
        bucketNumbers.push(i);
    }

    log().trace('Beginning collection of %s buckets.', bucketNumbers.length);

    // Perform a collection cycle on the bucket numbers
    collectBuckets(bucketNumbers, function(err) {
        log().trace('Completed collection cycle.');

        // Mark that this collection cycle has completed, whether or not there was an error
        currentConcurrentCollectionCount--;
        return callback(err);
    });
};

/**
 * Collects the given array of bucket numbers until they are empty, or "stolen". This process will skip over buckets that are
 * in the process of being collected. Therefore, if all buckets in the array are currently undergoing collection, this will not
 * actually perform any collections. This process will only attempt to collect one bucket at a time in order given in the array.
 *
 * @param  {Number[]}   bucketNumbers   The buckets that should be collected by this cycle.
 * @param  {Function}   callback        Invoked when the collection cycle completes.
 * @param  {Object}     callback.err    An error that occurred, if any
 * @param  {Object[]}   [errs]          Maintains a list of errors that occurred during collection. Used recursively.
 */
var collectBuckets = module.exports.collectBuckets = function(bucketNumbers, callback, errs) {
    errs = errs || [];
    if (!bucketNumbers || bucketNumbers.length === 0) {
        // Return the first error, if there was any.
        return callback(errs[0]);
    }

    var bucketNumber = bucketNumbers.shift();
    _collectBucket(bucketNumber, function(err) {
        if (err) {
            log().warn({'err': err, 'bucketNumber': bucketNumber}, 'Error collecting aggregate bucket.');
            Telemetry.incr('collection.error.count');
            errs.push(err);
        }

        collectBuckets(bucketNumbers, callback, errs);
    });
};

/**
 * Collect the provided bucket number until it is empty (or "stolen"). If the bucket is already being collected, this will
 * effectively do nothing.
 *
 * @param   {Number}    bucketNumber        The number of the bucket to collect.
 * @param   {Function}  callback            Invoked when collection is complete.
 * @param   {Object}    callback.err        An error that occurred, if any.
 * @api private
 */
var _collectBucket = function(bucketNumber, callback) {
    if (shuttingDown) {
        log().info('Aborting activity bucket collection of bucket %s as shutdown is in progress.', bucketNumber);
        return callback();
    }

    log().trace('Attempting collection of bucket number %s', bucketNumber);
    // Try and acquire a lock on the bucket to collect the next batch
    ActivityCluster.acquireBucket(bucketNumber, ActivitySystemConfig.getConfig().collectionExpiry, function(err, lockId) {
        if (err) {
            return callback(err);
        } else if (lockId) {
            // We acquired the lock, perform a collection iteration
            _collectBucketBatch(bucketNumber, ActivitySystemConfig.getConfig().collectionBatchSize, function(collectionErr, empty) {
                // We want to ensure we release the bucket, whether we received an error or not
                ActivityCluster.releaseBucket(bucketNumber, lockId, function(releaseErr, hadLock) {
                    if (collectionErr) {
                        return callback(collectionErr);
                    } else if (releaseErr) {
                        // If there was an error releasing the lock, worst case scenario would be that the lock eventually expires
                        // and a cluster node picks it up soon after that and continues processing.
                        return callback(releaseErr);
                    }

                    if (!hadLock) {
                        // This means that the lock expired before we finished collecting, which likely means the lock expiry
                        // is not configured high enough for the collection batch size. Send an error, because it will almost
                        // certainly end up in a noticeable degradation of user experience.
                        log().error({
                            'collectionExpiry': ActivitySystemConfig.getConfig().collectionExpiry,
                            'collectionBatchSize': ActivitySystemConfig.getConfig().collectionBatchSize
                        }, 'The bucket lock expired before we finished collecting a batch of activities. This probably means that ' +
                           'it takes longer than the "collectionExpiry" number of seconds to collect "collectionBatchSize" number of ' +
                           'activities. Consider either increasing "collectionExpiry" or decreasing "collectionBatchSize" to fix this ' +
                           'problem.');
                    }

                    if (empty) {
                        // The bucket is now empty, return to the caller
                        return callback();
                    }

                    // The bucket isn't empty, try another collection iteration
                    return _collectBucket(bucketNumber, callback);
                });
            });
        } else {
            // We could not acquire a lock, someone else came around and managed to snag the bucket
            return callback();
        }
    });
};

/**
 * Collect and process a certain amount of routed activities from the given bucket. This method is *not safe* in the sense
 * that it does not try and first acquire a lock on the bucket. Do not use this directly, instead use `collectBucket` which
 * in turn uses this method with locks.
 *
 * Collecting and processing activities in this method goes through the following steps:
 *
 *  1. Get the next batch of queued activities in the bucket. The number of activities processed is determined by `limit`.
 *
 *  2.  Expand the activities retrieved into all the potential "aggregate keys" that it could match. When expanding, a
 *      preliminary process of aggregation occurs where we collect aggregates within just the set of activities we
 *      fetched from the queue. See `_createAggregates` for more information.
 *
 *  3.  Get the status of all aggregate keys we expanded. This helps us identify which aggregates are "active" (i.e., have
 *      received matching activities and has not expired). It also helps us identify expired aggregates, in which case we
 *      can delete all their aggregated data (status and entities).
 *
 *  4.  For all expired aggregates, delete their status entry and all their previously aggregated entities. For more information
 *      on expiry, see `_isExpired`.
 *
 *  5.  For all *active* aggregates, fetch all the aggregated entities that are stored for them. This ensures that the activities
 *      current history can carry forward when this aggregate is redelivered.
 *
 *  6.  Merge all the aggregates with their aggregated entities fetched in step #5, and identify which aggregates will be
 *      delivered as activities. Aggregates that will be delivered are determined by:
 *
 *          *   Identify which activities belong to active aggregates, merge the data and mark them to be delivered (i.e., the activity
 *              becomes "claimed"). In this case, the `lastActivity` id of the aggregate is recorded so it may be deleted (because it
 *              was replaced by the new aggregate)
 *          *   Identify which activities do not belong to any active aggregate and mark them to be delivered, but only once per route
 *
 *  7.  Save all the new aggregate entities that were found in the collected batch of queued activities. This ensures that when
 *      the next activity comes along and matches those aggregates, that historical information is there to carry forward.
 *
 *  8.  Iterate over all the aggregates that are marked for delivery, and generate the activity object based on their aggregated
 *      entities and metadata (publish date, verb, activityType, etc...)
 *
 *  9.  Persist the new activities to their routes
 *
 *  10. Delete all the activities that were being tracked as active aggregates. We recorded which activities need to be deleted
 *      during step #6
 *
 *  11. Update the status of all aggregates. At the very least to indicate they were just touched by an activity. For aggregates that
 *      actually resulted in a delivered activity, they will also be given a `lastActivity` id that will be used to delete the
 *      activity we just delivered if a new activity matches the aggregate
 *
 *  12. Delete the queued activities that we just processed so that they don't get reprocessed.
 *
 * @param   {Number}    bucketNumber        The bucket to process.
 * @param   {Number}    limit               The number of routed activities to collect, process and deliver.
 * @param   {Function}  callback            Invoked when the batch has been processed.
 * @param   {Object}    callback.err        An error that occurred, if any.
 * @api private
 */
var _collectBucketBatch = function(bucketNumber, limit, callback) {
    var collectionStart = Date.now();
    log().trace('Collecting batch of %s entries from bucket number %s.', limit, bucketNumber);

    // Step #1: Get the next batch of queued activities to process
    ActivityDAO.getQueuedActivities(bucketNumber, limit, function(err, routedActivities) {
        if (err) {
            return callback(err);
        }

        // These routed activities will be deleted by the internal id after they've been processed
        var queuedActivitiesToDelete = _.keys(routedActivities);
        var numCollected = queuedActivitiesToDelete.length;
        if (numCollected === 0) {
            // No more to process, so stop and report that we're empty.
            return callback(null, true);
        }

        // Step #2: Explode the routed activities into their potential aggregates, according to their configured pivot points
        var allAggregates = _createAggregates(_.values(routedActivities));
        var allAggregateKeys = _.keys(allAggregates);

        // Step #3: Get all aggregate statuses to determine which ones are expired and which are active. Expired aggregates
        // should be deleted, while active aggregates should be merged and redelivered
        ActivityDAO.getAggregateStatus(allAggregateKeys, function(err, statusByAggregateKey) {
            if (err) {
                return callback(err);
            }

            // Figure out which aggregates are "active" (have an activity in its aggregate) and "expired" (no new activities in the aggregate before expiry time)
            var activeAggregates = {};
            var expiredAggregates = {};
            allAggregateKeys.forEach(function(aggregateKey) {
                var aggregate = allAggregates[aggregateKey];
                var status = statusByAggregateKey[aggregateKey];
                if (status && _isExpired(status, allAggregates[aggregateKey].published)) {
                    expiredAggregates[aggregateKey] = true;
                } else if (status) {
                    activeAggregates[aggregateKey] = true;
                }
            });

            // Note: We need to delete aggregated entities and save them here within the collection chain to avoid nuking undelivered
            // entities. If we saved aggregated entities during the routing phase and only deleted them here, it would save us a write
            // as we wouldn't have to write them to the queue, but it exposes a race condition where entities that are saved between
            // getAggregateStatus (above) and deleteAggregatedEntities (below) will be deleted before delivery.

            // Step #4: Delete all the expired aggregates before aggregating new stuff
            ActivityDAO.deleteAggregates(_.keys(expiredAggregates), function(err) {
                if (err) {
                    return callback(err);
                }

                // Step #5: Retrieve all entities that are aggregated within the active aggregates so they can be collected into redelivered activities
                ActivityDAO.getAggregatedEntities(_.keys(activeAggregates), function(err, fetchedEntities) {
                    if (err) {
                        return callback(err);
                    }

                    /*!
                     * Step #6:
                     *
                     * Here we choose which aggregates need to be wrapped up into an activity and delivered to the activity stream. This is
                     * rather difficult to get right. These are the rules implemented below:
                     *
                     *  For a given activity:
                     *
                     * 1.   If an aggregate was previously active for it, it will "claim" the activity, and a new activity will be re-delivered
                     *      to the route with the aggregated data. If there was a previous activity delivered for the aggregate in that stream,
                     *      that activity is deleted from the stream.
                     *
                     * 2.   If there are multiple active aggregates for the activity in the stream, all should be delivered as they all have
                     *      been updated. This means that if an activity has multiple aggregates, you could update the branched activities
                     *      for *both* aggregates (e.g., content-share). "Branching" means that 2 separate aggregations have started and are
                     *      tracking two separate activities in a feed.
                     *
                     *      How can an activity fall into two branches? Using content-share as an example, let's say we have 2 live
                     *      aggregates:
                     *
                     *          Aggregate #1: "Branden shared 5 content items with OAE Team"
                     *          Aggregate #2: "Branden shared Syllabus with 5 Users and Groups"
                     *
                     *      Now, if Branden decides to share "Syllabus" with "OAE Team", you have matched both of these aggregates, and both
                     *      "branches" will be updated and moved to the top of the feed.
                     *
                     * 3.   An activity is only delivered for an inactive aggregate if the activity was not "claimed" for the route by an
                     *      active aggregate. This would make sure that we don't redliver an active aggregate, AND deliver the single activity
                     *      (e.g., "Branden shared Syllabus with OAE Team") for the same route.
                     *
                     * 4.   If no active aggregates claim an activity, and there are multiple inactive aggregates (e.g., the activity type has
                     *      multiple "pivot points"), then one single activity is delivered for all of them. This is necessary to ensure that
                     *      the "lastActivityId" is recorded properly for both aggregates, so if either of those inactive aggregates become
                     *      active later (i.e., another activity comes along and matches it), the previous activity can be properly deleted by
                     *      either of the aggregates.
                     *
                     * 5.   If an activity gets "duplicated", it should not result in multiple activities, it should instead just update the
                     *      publish date and the content entities of the original activities.
                     */

                    // Mark all active aggregates to be delivered and create a new activity ID for them, as they will "branch" into
                    // their activity.
                    var aggregatesToDeliver = {};
                    var activitiesToDelete = {};
                    var claimedRouteActivities = {};
                    allAggregateKeys.forEach(function(aggregateKey) {
                        var aggregate = allAggregates[aggregateKey];
                        if (activeAggregates[aggregateKey] || aggregate.activityIds.length > 1) {
                            var status = statusByAggregateKey[aggregateKey];
                            
                            // Mark this to be delivered and assign it an activity id
                            aggregatesToDeliver[aggregateKey] = true;
                            aggregate[ActivityConstants.properties.OAE_ACTIVITY_ID] = ActivityDAO.createActivityId(aggregate.published);

                            // Mark these activities for this route as being claimed by an active aggregate
                            claimedRouteActivities[aggregate.route] = claimedRouteActivities[aggregate.route] || {};
                            aggregate.activityIds.forEach(function(activityId) {
                                claimedRouteActivities[aggregate.route][activityId] = true;
                            });

                            // If this was previously delivered, delete the previous activity
                            if (status && status.lastActivity) {
                                activitiesToDelete[aggregate.route] = activitiesToDelete[aggregate.route] || {};
                                activitiesToDelete[aggregate.route][status.lastActivity] = true;
                            }
                        }
                    });

                    // Second, for aggregates that are not active, determine if they can be delivered
                    allAggregateKeys.forEach(function(aggregateKey) {
                        var aggregate = allAggregates[aggregateKey];
                        // Any aggregate that had more than 1 activityId was claimed as active. We can safely just use the first
                        // activityId to get the activityId that represents this aggregate
                        var activityId = aggregate.activityIds[0];
                        var isActivityClaimed = (claimedRouteActivities[aggregate.route] && claimedRouteActivities[aggregate.route][activityId]);
                        if (!activeAggregates[aggregateKey] && !isActivityClaimed) {
                            // If this route has not received an aggregate, then we deliver the non-active one(s). In the event that
                            // there are multiple non-active aggregates, a duplicate activity will not be fired because we flatten and
                            // maintain a set while generating activities later.
                            aggregatesToDeliver[aggregateKey] = true;
                            aggregate[ActivityConstants.properties.OAE_ACTIVITY_ID] = activityId;
                        }
                    });

                    // Step #7: Save the aggregated entities stored in the current batch of aggregates
                    ActivityDAO.saveAggregatedEntities(allAggregates, function(err) {
                        if (err) {
                            return callback(err);
                        }

                        // Step #8: Create the actual activities to route
                        var numDelivered = 0;
                        var visitedActivities = {};
                        var activityStreamUpdates = {};
                        _.keys(aggregatesToDeliver).forEach(function(aggregateKey) {
                            var aggregate = allAggregates[aggregateKey];

                            // Construct the activities to deliver
                            var activityType = aggregate[ActivityConstants.properties.OAE_ACTIVITY_TYPE];
                            var verb = aggregate.verb;
                            var published = aggregate.published;

                            // Refresh the entities with the freshly fetched set, which has all the entities, not those just in this collection
                            // We need to make sure we override with the queued entities and not the freshly fetched ones since they may have been
                            // updated since original aggregation.
                            if (fetchedEntities[aggregateKey]) {
                                aggregate.addActors(fetchedEntities[aggregateKey].actors);
                                aggregate.addObjects(fetchedEntities[aggregateKey].objects);
                                aggregate.addTargets(fetchedEntities[aggregateKey].targets);
                            }

                            // Make sure that we don't deliver an identical activity to the same stream twice. This can potentially
                            // happen when an activity type has multiple pivots that were inactive prior to this activity (e.g., content-share)
                            var activityId = null;
                            var flattenedActivity = _flattenActivity(aggregate);
                            if (visitedActivities[flattenedActivity]) {
                                // We assign the previous activity id to the aggregate so that we can update the aggregate status to know that
                                // any new activities for this aggregate should replace its existing activity
                                aggregate[ActivityConstants.properties.OAE_ACTIVITY_ID] = visitedActivities[flattenedActivity];
                                return;
                            } else {
                                // This activity is not a duplicate, assign and record a new activityId
                                activityId = ActivityDAO.createActivityId(aggregate.published);
                                aggregate[ActivityConstants.properties.OAE_ACTIVITY_ID] = activityId;
                                visitedActivities[flattenedActivity] = activityId;
                            }

                            // Create the entities for the delivered activity
                            var actor = _createActivityEntity(_.values(aggregate.actors));
                            var object = _createActivityEntity(_.values(aggregate.objects));
                            var target = _createActivityEntity(_.values(aggregate.targets));

                            activityStreamUpdates[aggregate.route] = activityStreamUpdates[aggregate.route] || {};
                            activityStreamUpdates[aggregate.route][activityId] = new Activity(activityType, activityId, verb, published, actor, object, target);
                            numDelivered++;
                        });

                        // Step #9: Deliver the new activities to the streams
                        ActivityDAO.deliverActivities(activityStreamUpdates, function(err) {
                            if (err) {
                                return callback(err);
                            }

                            // Collection date is marked as the date/time that the aggregate gets delivered
                            var collectionDate = Date.now();

                            // Record how long it took for these to be delivered
                            _.keys(activityStreamUpdates).forEach(function(route) {
                                _.keys(activityStreamUpdates[route]).forEach(function(activityId) {
                                    var activity = activityStreamUpdates[route][activityId];
                                    Telemetry.appendDuration('delivery.time', activity.published);
                                });
                            });

                            // The activitiesToDelete hash values should actually be arrays of unique activity ids, not "<activity id>: true" pairs.
                            _.keys(activitiesToDelete).forEach(function(route) {
                                activitiesToDelete[route] = _.keys(activitiesToDelete[route]);
                            });

                            // Step #10: Delete the old activities that were replaced by aggregates
                            ActivityDAO.deleteActivities(activitiesToDelete, function(err) {
                                if (err) {
                                    return callback(err);
                                }

                                // Determine how to update all the aggregate statuses
                                var statusUpdates = {};
                                allAggregateKeys.forEach(function(aggregateKey) {
                                    var aggregate = allAggregates[aggregateKey];
                                    statusUpdates[aggregateKey] = {
                                        'lastUpdated': aggregate.published,
                                        'lastCollected': collectionDate
                                    };

                                    if (!activeAggregates[aggregateKey]) {
                                        // This aggregate was not previously active, so mark its creation date at the beginning of the first activity
                                        statusUpdates[aggregateKey].created = aggregate.published;
                                    }

                                    // Mark the last activity for each aggregate. This ensures that when a new activity gets added to the aggregate, we can
                                    // delete the previous one.
                                    if (aggregate[ActivityConstants.properties.OAE_ACTIVITY_ID]) {
                                        statusUpdates[aggregateKey].lastActivity = aggregate[ActivityConstants.properties.OAE_ACTIVITY_ID];
                                    }
                                });

                                // Step #11: Update the activity statuses, indicating they have just been updated and collected, where applicable
                                ActivityDAO.updateAggregateStatus(statusUpdates, function(err) {
                                    if (err) {
                                        return callback(err);
                                    }

                                    // Step #12: Remove the queued routed activities that were just delivered / aggregated so they don't get re-processed
                                    ActivityDAO.deleteQueuedActivities(bucketNumber, numCollected, function(err) {
                                        if (err) {
                                            return callback(err);
                                        }

                                        // Fire an event that we have successfully delivered these individual activities
                                        var deliveredActivities = {};
                                        var deliveredNotifications = {};
                                        _.each(routedActivities, function(routedActivity) {
                                            if (routedActivity) {
                                                if (routedActivity.route.slice(-13) === '#notification') {
                                                    // The userId is everything up until the last #
                                                    var userId = routedActivity.route.split('#').slice(0, -1).join('#');
                                                    deliveredNotifications[userId] = deliveredNotifications[userId] || [];
                                                    deliveredNotifications[userId].push(routedActivity.activity);
                                                } else {
                                                    deliveredActivities[routedActivity.route] = deliveredActivities[routedActivity.route] || [];
                                                    deliveredActivities[routedActivity.route] = routedActivity.activity;
                                                }
                                            }
                                        });

                                        if (!_.isEmpty(deliveredActivities)) {
                                            ActivityEmitter.emit(ActivityConstants.events.DELIVERED_ACTIVITIES, deliveredActivities);
                                        }

                                        if (!_.isEmpty(deliveredNotifications)) {
                                            ActivityEmitter.emit(ActivityConstants.events.DELIVERED_NOTIFICATIONS, deliveredNotifications);
                                        }

                                        Telemetry.appendDuration('collection.time', collectionStart);
                                        Telemetry.incr('collected.count', numCollected);
                                        Telemetry.incr('delivered.count', numDelivered);

                                        return callback();
                                    });
                                });
                            });
                        });
                    });
                });
            });
        });
    });
};


/**
 * Explode the given routed activities into all potential aggregates. An aggregate is a permutation of a routed activity that
 * further keys each by the pivot points by which the activity can be aggregated over a period of time.
 *
 * The routed activities are an array of following form:
 *
 * [
 *  {
 *      'route': <route>,
 *      'activity': <Activity>
 *  },
 *  { ... }
 * ]
 *
 * Where the route specifies the route to which the activity should be delivered, and the activity is the activity to deliver.
 *
 * The result will be an object representing the aggregation of all the routed activities in the list, keyed by the aggregate key. An
 * example aggregation of an activity that pivots on actor and had 3 matching aggregates in the array would be:
 *
 *  {
 *       '<aggregateKey>': {
 *           'route': '...',
 *           'oae:activityType': '...',
 *           'activityIds': [ '<activityId0>', ... ],
 *           'verb': '...',
 *           'published': '...',
 *           'actors': {
 *               '<actorKey0>': <ActivityEntity (actor)>
 *           },
 *           'objects': {
 *               '<objectKey0>': <ActivityEntity (object)>,
 *               '<objectKey1>': <ActivityEntity (object)>,
 *               '<objectKey2>': <ActivityEntity (object)>
 *           },
 *           'targets': {}
 *       }
 *  }
 *
 *  @see #ActivityAggregate model object for more details
 *
 * @param   {Object[]}  routedActivities    An array of activities along with the route to which they should be delivered. See summary for more information.
 * @return  {Object}                        An object representing the potential aggregates of the collected batch of activities.
 * @api private
 */
var _createAggregates = function(routedActivities) {
    var aggregates = {};
    routedActivities.forEach(function(routedActivity) {
        // A routedActivity could be null if the contents failed to parse (corrupt?). Just skip over it.
        if (!routedActivity) {
            return;
        }

        var route = routedActivity.route;
        var activity = routedActivity.activity;
        var activityType = activity[ActivityConstants.properties.OAE_ACTIVITY_TYPE];
        var activityId = activity[ActivityConstants.properties.OAE_ACTIVITY_ID];

        // Build the entity keys which will be used to create the aggregate key
        var actorKey = _createEntityKey(activity.actor);
        var objectKey = _createEntityKey(activity.object);
        var targetKey = _createEntityKey(activity.target);

        // Determine how this activity will be grouped (a.k.a., pivot points) for aggregation
        var activityTypes = ActivityRegistry.getRegisteredActivityTypes();
        var groupBy = (activityTypes[activityType]) ? activityTypes[activityType].groupBy : [];

        // Ensure we atleast have the "all" aggregate, which means we don't get duplicate activities within the same aggregation
        // period.
        if (groupBy.length === 0) {
            groupBy = [{
                'actor': true,
                'object': true,
                'target': true
            }];
        }

        // For each potential grouping, create an "aggregate key", which will be used to determine if new activity deliveries
        // match with the same key.
        groupBy.forEach(function(pivot) {
            var pivotActorKey = (pivot.actor) ? actorKey : '';
            var pivotObjectKey = (pivot.object) ? objectKey : '';
            var pivotTargetKey = (pivot.target) ? targetKey : '';

            // The aggregate key is of the following format: "content-create#u:oae:mrvisser#user:u:oae:mrvisser##
            var aggregateKey = util.format('%s#%s#%s#%s#%s', activityType, route, pivotActorKey, pivotObjectKey, pivotTargetKey);

            // This process of collecting actors, objects, targets and activities is in some respect "in-memory aggregation". It
            // helps to use this to determine ahead of time if there are a few activities within this batch of routed activities
            // that already match. It helps us deliver an aggregate right away to the route, rather than accidentally delivering
            // individual activities from within a batch.
            if (!aggregates[aggregateKey]) {
                aggregates[aggregateKey] = new ActivityAggregate(activityType, route, activity.verb, activity.published);
            }

            var aggregate = aggregates[aggregateKey];
            var suppressAggregate = true;
            var isNew = false;

            // Below, we suppress the aggregate only if it does not contribute any new entity to an existing aggregate. This
            // allows us to avoid aggregating exact duplicates from within the batch and making it look like it is a live
            // aggregate.

            if (activity.actor) {
                isNew = aggregate.updateActor(actorKey, activity.actor);
                if (isNew) {
                    suppressAggregate = false;
                }
            }

            if (activity.object) {
                isNew = aggregate.updateObject(objectKey, activity.object);
                if (isNew) {
                    suppressAggregate = false;
                }
            }

            if (activity.target) {
                isNew = aggregate.updateTarget(targetKey, activity.target);
                if (isNew) {
                    suppressAggregate = false;
                }
            }

            // Ensure we record the most recent occurance of an activity
            aggregate.published = activity.published;

            // Only make this aggregate look like an active aggregate if a second one actually contributed a new entity
            if (!suppressAggregate) {
                aggregate.activityIds.push(activityId);
            }
        });
    });

    return aggregates;
};

/**
 * Given an array of activity entities, return a new top-level activity entity representing how it should be modeled in an activity
 * stream.
 *
 * @param   {ActivityEntity[]}  entities        The activity entities to transform.
 * @return  {ActivityEntity}                    An individual activity entity that represents the collection of entities.
 * @api private
 */
var _createActivityEntity = function(entities) {
    if (!entities) {
        return undefined;
    } else if (!entities.length) {
        return undefined;
    } else if (entities.length === 1) {
        return entities[0];
    }

    var ext = {};
    ext[ActivityConstants.properties.OAE_COLLECTION] = entities;
    return new ActivityEntity('collection', undefined, {'ext': ext});
};

/**
 * Flatten an aggregate into a string identity that allows us to determine if the activity that will be created by an aggregate
 * is identical to another. This can be used to maintain a hash of identities to quickly determine whether or not an activity
 * should be delivered.
 *
 * @param   {Object}    aggregate   The aggregate from which to deliver an activity identity
 * @return  {String}                A string identity that can be used to determine if one activity is identical to another
 * @api private
 */
var _flattenActivity = function(aggregate) {
    var route = aggregate.route;
    var activityType = aggregate[ActivityConstants.properties.OAE_ACTIVITY_TYPE];

    // Create a multi-key of all the actors, objects and targets so they are deterministic
    var actorsKeys = [];
    _.values(aggregate.actors).forEach(function(actor) {
        actorsKeys.push(_createEntityKey(actor));
    });
    actorsKeys = actorsKeys.sort().join(',');

    var objectsKeys = [];
    _.values(aggregate.objects).forEach(function(object) {
        objectsKeys.push(_createEntityKey(object));
    });
    objectsKeys = objectsKeys.sort().join(',');

    var targetsKeys = [];
    _.values(aggregate.targets).forEach(function(target) {
        targetsKeys.push(_createEntityKey(target));
    });
    targetsKeys = targetsKeys.sort().join(',');

    // Generate the identity key for the activity described by the aggregate. It looks like:
    // content-create#u:oae:mrvisser#user:u:oae:mrvisser#c:oae:jfEIop-,c:oae:PVOsdf43j##
    return util.format('%s#%s#%s#%s#%s', activityType, route, actorsKeys, objectsKeys, targetsKeys);
};

/**
 * Create a unique string representation from the given entity. Looks something like: user:u:oae:mrvisser
 *
 * If the entity is not specified, returns `ENTITY_KEY_EMPTY` as a placeholder for the entity key.
 *
 * @param   {ActivityEntity}    entity      The entity for which to create an entity key.
 * @return  {String}                        A unique string representation of the entity.
 */
var _createEntityKey = function(entity) {
    return (entity) ? util.format('%s:%s', entity.objectType, entity[ActivityConstants.properties.OAE_ID]) : ENTITY_KEY_EMPTY;
};

/**
 * Determine if the aggregate described by aggregateStatus is considered to be expired at the provided published date.
 *
 * An aggregate is expired when the following conditions hold true:
 *
 *  a) The last update that was made to the aggregate has been collected by a collection routine; and
 *  b) It has not been longer than the configured `aggregateIdleExpiry` seconds since the last activity matched the aggregate; and
 *  c) The aggregate has not been active for longer than the configured `aggregateMaxExpiry` time.
 *
 * @param   {Object}    aggregateStatus         The aggregate status entry.
 * @param   {Number}    published               The published date (in millis since the epoch) that the next activity occurred.
 * @param   {Boolean}                           Whether or not the aggregate is expired.
 */
var _isExpired = function(aggregateStatus, published) {
    var aggregateIdleExpiryInMs = ActivitySystemConfig.getConfig().aggregateIdleExpiry * 1000;
    var aggregateMaxExpiryInMs = ActivitySystemConfig.getConfig().aggregateMaxExpiry * 1000;

    var lastUpdateWasCollected = (aggregateStatus.lastCollected && aggregateStatus.lastCollected > aggregateStatus.lastUpdated);
    var lastUpdateIsIdleExpired = ((published - aggregateStatus.lastUpdated) > aggregateIdleExpiryInMs);
    var createdMaxIsExpired = ((published - aggregateStatus.created) > aggregateMaxExpiryInMs);
    return (lastUpdateWasCollected && (lastUpdateIsIdleExpired || createdMaxIsExpired));
};

////////////////////
// INTERNAL MODEL //
////////////////////

/**
 * A model object that represents the data associated to multiple activities for the same route aggregated together.
 *
 * @param  {String}     activityType    The type of the activities that were aggregated together
 * @param  {String}     route           The destination route for the activities that were aggregated together
 * @param  {String}     verb            The verb of the activities that were aggregated together
 * @param  {Number}     published       The latest timestamp (millis since the epoch) of the activities that were aggregated together
 * @api private
 */
var ActivityAggregate = function(activityType, route, verb, published) {
    var that = {};
    that[ActivityConstants.properties.OAE_ACTIVITY_TYPE] = activityType;
    that.route = route;
    that.verb = verb;
    that.published = published;
    that.activityIds = [];
    that.actors = {};
    that.objects = {};
    that.targets = {};

    /*!
     * Update the existing (if any) actor in the aggregate with the given actor. If the actor did not exist on the aggregate it will
     * be added.
     *
     * @param  {String}     actor   The unique key of the actor object
     * @param  {Object}     actor   The actor object to update
     * @return {Boolean}            `true` if the actor did not exist. `false` if there was an update.
     */
    that.updateActor = function(actorKey, actor) {
        var isNew = !that.actors[actorKey];
        that.actors[actorKey] = actor;
        return isNew;
    };

    /*!
     * Update the existing (if any) object in the aggregate with the given object. If the object did not exist on the aggregate it
     * will be added.
     *
     * @param  {String}     objectKey   The unique key of the object object
     * @param  {Object}     object      The object object to update
     * @return {Boolean}                `true` if the object did not exist. `false` if there was an update.
     */
    that.updateObject = function(objectKey, object) {
        var isNew = !that.objects[objectKey];
        that.objects[objectKey] = object;
        return isNew;
    };

    /*!
     * Update the existing (if any) target in the aggregate with the given target. If the target did not exist on the aggregate it
     * will be added.
     *
     * @param  {String}     targetKey   The unique key of the target target
     * @param  {Object}     target      The target target to update
     * @return {Boolean}                `true` if the target did not exist. `false` if there was an update.
     */
    that.updateTarget = function(targetKey, target) {
        var isNew = !that.targets[targetKey];
        that.targets[targetKey] = target;
        return isNew;
    };

    /*!
     * Add the given hash of actors to the given collection of actors. If any actors in the given set are already contained, they
     * are not added/updated to the current set of actors.
     *
     * @param  {Object}    actors      An object, keyed by the unique entity key, whose value is the actor to add to the current set of actors
     */
    that.addActors = function(actors) {
        if (actors) {
            that.actors = _.extend(actors, that.actors);
        }
    };

    /*!
     * Add the given hash of objects to the given collection of objects. If any objects in the given set are already contained, they
     * are not added/updated to the current set of objects.
     *
     * @param  {Object}    objects      An object, keyed by the unique entity key, whose value is the object to add to the current set of objects
     */
    that.addObjects = function(objects) {
        if (objects) {
            that.objects = _.extend(objects, that.objects);
        }
    };

    /*!
     * Add the given hash of targets to the given collection of targets. If any targets in the given set are already contained, they
     * are not added/updated to the current set of targets.
     *
     * @param  {Object}    targets      An object, keyed by the unique entity key, whose value is the target to add to the current set of targets
     */
    that.addTargets = function(targets) {
        if (targets) {
            that.targets = _.extend(targets, that.targets);
        }
    };

    return that;
};

